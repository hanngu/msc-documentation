\subsection{Evaluation Criteria}
\label{subsec:evaluationCriteraCohen}

To evaluate the proposed method and this project as a whole, some evaluation criteria must be defined. Why we are doing our research, why our views and methods are a step forward, how completely they are implemented by our programs, how these programs work, whether their performance is likely to increase or has reached a limit (and why), and what problems we encounter ate each stage of our research. Evaluation should be a mechanism of progress both within and across AI research. Evaluation provides a driving force to the research cycle. Evaluation provides a basis for congestions of knowledge. Without evaluation: cannot replicate results. Evaluation: convince research community that ideas are worthwhile, that they work and how. 

Evaluation is an issue discussed a lot in the field of artificial intelligence. Where other sciences has specific aspects of research to be presented, empirical AI has not comparable standards, in this effort has \citet{cohen88}, as initiated in Section\vref{sec:structuredLiteratureReview}, introduced a five-stage mode to use for evaluating AI systems: 
\begin{enumerate}
\item Refine the research topic to a task and a define a view of how to accomplish the task
\item Refined the view to a specific method
\item Develop a program to implement the method
\item Design experiments to test the program
\item Run the experiments
\end{enumerate} 
 
The task is UTRP using SI. 
And the goal is to produce better results than other research, regarding the performance criteria.  
The hypothesis is that, implementation from PSO: CA: act more random in beginning - compensate for the original ACO algorithm's weakness of frequently getting stuck at a local optima. Implemented the program to test the method, and run experiments to evaluate against performance criteria.

One attempt to combine different methods from swarm intelligence - include features from other swarm inspired methods. Neo4j.
