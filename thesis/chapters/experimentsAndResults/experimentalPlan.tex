\section{Experimental Plan}
\label{sec:expPlan}


\subsection{Parameter Settings}
\label{subsec:parameterSettings_plan}
Metaheuristics, like ACO, requires good initial parameters to solve concrete problems optimally. In order to study the effect of the variation of the parameters, will we conduct a series of experiments to find the most optimal algorithm parameters concerning the performance criteria. As mentioned in Section \vref{sec:relatedWork}, refers several authors to their parameter settings experiments as a product of ``trial and error'', without presenting the parameter values tested. For contributing to the field and providing a starting point for future research, will this thesis include a complete review of the conducted experiment. In addition will the experiment help establish Research Question \vref{itm:2a}, by studying the effect of the variation on the additional PSO and BCO parameters.  

%The exact shape of any normal curve is totally determined by its mean and standard deviation. %Therefore, if we know the mean and standard deviation of a statistic, we can find the mean and standard deviation of the sampling distribution of the statistic

%For most of the parameters the different parameter values will only be tested a limited number of times, and a statistical analysis will not be provided. This is because a complete analysis of the parameter settings is beyond the scope of this thesis, and the results of the tests without a statistical analysis should only be considered as indicative. The parameters directly linked to features of PSO and BCO will be tested more thoroughly and a statistical analysis will be provided, because this will help us establish Research Question \vref{itm:2a}. 

\subsection{Performance Comparison}
\label{subsec:performanceComparison_plan}
\emph{\color{blue}TODO:} %Features are added from PSO/BSO, and the optimal algorithm parameters are found. 
We will in this stage test the system's performance, comparing it with the respective results, concerning the performance criteria from section \vref{sec:performanceCriteria}. %\emph{\color{red}ACO has a known limitation of being stuck on local optima, and the time/space(?) complexity is high.} We will therefore 
%add features from PSO - inertial weight, more random in the start and accepting more solutions, knowledge about the best global solution for every iteration, keep the best current solution. After adding SI features from PSO / BSO, 
The aim with this test is to compare our results with ref.
%\begin{enumerate}

%\item Add features from PSO - inertial weight, more random in the start and accepting more solutions, knowledge about the best global solution for every iteration, keep the best current solution.

%\item Evaluate results with only the ACO implementation, on Mandl's network and comparing it with the results from other ACO implementations. The aim with this is to test solution quality and check if we need to change the algorithm / add features from other SI algorithms in order to improve the results, in order to answer research question 2. \emph{\color{red} TODO: Known limitations of ACO. Stuck on local optima, time complexity.}

%\item After adding SI features from PSO / BSO, we will check whether it is efficient to combine different swarm intelligence methods' attributes to get better results concerning the vehicle routing problem, in order to answer the research question 2 a.

%\item And to test whether Neo4J is suited, and if Neo4J Dijkstra's or A* is best concerning run times? The aim is to see if the potential advantages for using a graph database in our implementation, and if this is useful in the optimization process, in order to answer research question 3 and 3 a.

The algorithm will be tested with different number of routes.


%\end{enumerate}

\subsection{Scalability Experiments}
\emph{\color{blue} TODO: Time and Space Complexity.}
The programs are rarely informative if they are designed to run on a single example - Therefore we will test algorithm on other, larger, networks, to check whether it is general and not just optimized for Mandl. (Mumford)

%In order to test whether the method is general and not tuned in to run on a single example, we will run the algorithm on other, larger, networks. (Mumford included larger networks.) Aim: Here we will test the time and space complexity.

Record our run times - test the efficiency of our algorithm.




