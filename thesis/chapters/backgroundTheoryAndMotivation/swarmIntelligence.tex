\section{Swarm Intelligence}
\label{sec:swarmIntelligence}
Swarm behavior is found in many different species in nature, including fish schools and flocks of birds. Many species practice swarm behavior for a a biological need to stay together, because predators usually attacks one individual, and not an entire flock. Swarm behavior is also found in social insects like ants, wasps, bees and termites. They collaborate on tasks for building nests, gathering food and organizing production. These social insect colonies have shown us that simple organisms can perform complex tasks by continuously interacting with each other. The colonies are highly distributed and self-organized, and they adapt well to changes in the environment. Swarm intelligence \citep{beni89} is a branch of artificial intelligence that is strongly influenced by the swarm behavior found in the nature, attempting to adapt these characteristics in intelligent computer systems. Swarm intelligence based algorithms, such as the ones described in \ref{subsec:aco}-\ref{subsec:pso}, are metheuristic optimization algorithms. Metaheuristic algorithms are typically used to find good solutions to combinatorial optimization problems, but they do not guarantee an optimal solution. 

\subsection{Ant Colony Optimization}
\label{subsec:aco}
In nature ants have proven to be extremely capable of finding an optimal or close to optimal route from the nest to a food source \citep{deneubourg90}. Ants communicate by leaving a pheromone trail that other ants smell, and will follow by a certain probability. Most ant species leave a pheromone trail when returning to the nest from an important food source, and the ants that decide to follow the same path also leave behind a pheromone trail. The more pheromone units on the trail (i.e. the more ants that have chosen the given path), the greater the probability that other ants will choose it. Because pheromone disappear over time, shorter paths will be favored over longer paths simply because shorter paths (usually) takes shorter time, resulting in more pheromone units. In nature ants are extremely adaptable to changes, and manage to find the shortest path even though an obstacle are added to the current shortest path. This is illustrated in Figure \ref{fig:ants}. To the left the ants have found an efficient route, in the middle an obstacle are added and the ants explore different paths, and to the right the ants have found the most efficient path around the obstacle. 

\begin{figure}[H]
  \centering
  \includegraphics[width=3in]{assets/maur.png}
  \caption{Ants adapting to change}
   \label{fig:ants}
\end{figure}

Ant Colony Optimization (ACO) is a class of graph representation based algorithms designed to optimize routing problems. The first description of an ACO algorithm, called Ant System (AS), was initially proposed by \citet{dorigo96}. The AS strategy developed by Dorigo tries to simulate the behavior of real ants by the fact that the pheromone values are updated by all ants that have build a solution. Algorithm \ref{algo:aco} shows the metaheuristic of ACO, as described by \citet{dorigo06}: \\
\begin{algorithm}
\caption{The Ant Colony Optimization Metaheuristic}
\label{algo:aco}
\begin{algorithmic}
\\ Set parameters, initialize pheromone trails
  \While{termination condition not met}
    \State ConstructAntSolutions
    \State ApplyLocalSearch (optional)
    \State UpdatePheromones
  \EndWhile
\end{algorithmic}
\end{algorithm}
~\\
The idea of ACO algorithms is to create a decentralized system with multiple agents. The agents influence each others decisions by using pheromones. In the beginning, before any distinct pheromone trail is laid, the ants' choices are random and thus they perform a broad search in the environment. This randomness will decrease over time as the pheromone trails becomes more distinct. The literature describes many different algorithms that can be classified as ACO algorithms, including the work of \citet{salehi-nezhad07}, \citet{tripathi09}, \citet{jiang10}, and \citet{dias14}. Most of the authors describe artificial ants with additional features to those found in nature, including vision and memory. The addition of these features is typically done to increase the performance of the algorithm and to overcome some of the known weaknesses of ACO, such as getting stuck at local optima.  
\emph{\color{blue}Her m√• det kanskje legges til noe om AS og ACS}

%TODO: SKRIVE OM ACO I RUTEOPTIMALISERING! 

\subsection{Bee Colony Optimization}
\label{subsec:BCO}
As described in \citet{lucic03}, bees are capable of performing a variety of complex tasks. Examples of these tasks are the collection and processing of nectar, which may be considered as extremely organized. The idea is that a bee that leaves the hive to gather nectar, flies to the hives so-called dance floor. The bees that have already found a good food source performs a ``dance'' at the dance floor to advertise that they have found a satisfying food source. The newly arrived bee choose one of the dancers and flies with it to the discovered food source. As stated in \citet{lucic03} the mechanism of deciding which bee to follow is not well understood, but it is considered that ``the recruitment among bees is always a function of the quality of the food source''. After the bee has gathered and returned the food to the hive, the bee has three options\citep{lucic03}:

\begin{enumerate}
  \item It can abandon the food source and return to the dance floor, and again become an uncommitted follower
  \item It can continue to gather nectar from the food source without recruiting nestmates
  \item It can return to the dance floor and dance, and thus recruit nestmates before returning to the food source
\end{enumerate}

Bee Colony Optimization (BCO) is a method that, like ACO, aims to create a decentralized optimization system with multiple agents, based on graph representation. The idea is to apply the collective intelligence of the food gathering process to an optimization system. Like a typical ACO algorithm, a typical BCO algorithm is inspired by the way bees acts in nature, but some additional features are added to the artificial bee, and some are removed. \citet{nikolic14} describes a BCO algorithm where the artificial bee only has two options after returning from a ``food source'': (1) abandon or (2) recruit. \citet{lucic03} gives the artificial bees attributes such as memory and perfect knowledge about the quantity of nectar collected by other bees. These modifications are done in order to make the algorithm more efficient, and thus more suitable of performing complex combinatorial problems.

\subsection{Particle Swarm Optimization}
\label{subsec:pso}
%[TODO]: Skrive om PSO
% http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=785511
As reported in \citet{shi99}, Particle Swarm Optimization (PSO) was first introduced by Eberhart and Kennedy in 1995. PSO is inspired by the social behavior of flocks (such as flocks of birds) and schools (such as fish schools), and the idea is to update the individuals velocity according to the individual's own experience and its companions' experience. This differs from other evolutionary computational algorithms, like Genetic Algorithms, which uses evolutionary operators to manipulate the individuals. The basic concept is that each individual moves with a certain velocity, and that this velocity is dynamically adjusted based on the experience. Each individual is a volume-less particle (i.e. a point) in the D-dimensional search space. The \textit{i}th particle position is represented as $X_i = (X_{i1},X_{i2}...X_{iD})$. Each particle knows its own best position so far (the position that gave the highest fitness value), represented as $P_i = (P_{i1},P_{i2}...P_{iD})$, and the best position \textit{g}, achieved among all the particles. These two positions are used to calculate the velocity, $V_i = (V_{i1},V_{i2}...V_{iD})$ ,  of the \textit{i}th particle \citep{shi99}: 
\newline
\newline
\centerline{$V_{id} = w * V_{id} + c_1 * rand() * (P_{id}-X_{id}) + c_2 * Rand() * (P_{gd}-X_{id})$}
\newline
\newline
where \textit{w} is a decreasing parameter called inertia weight brought to the PSO for balancing local and global search, $c_1$ and $c_2$ are two positive constants, and \textit{rand()} and \textit{Rand()} are two random functions in the range [0,1]. Figure \ref{fig:psoBeginning} illustrates how the particles in PSO tends to explore in the the early iterations of the algorithm, while figure \ref{fig:psoEnd} illustrates how the particles tend to act more organized and coordinated at the late iterations. The new position of the \textit{i}th particle is calculated as follows \citep{shi99}:
\newline
\newline
\centerline{$X_i = X_{id} + V_{id}$}
\newline
\newline
Because of the decreasing inertia weight, PSO may suffer from low global search ability at the end of the run, and thus risking getting stuck at a local optimum. This may result in the algorithm failing to find the required optimum when the problem to be solved is very complicated and complex \citep{shi99}. However, PSO has proven to solve NP hard problems including the Multi-Dimensional Knapsack Problem\citep{wan09}. 

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{assets/pso_diagram1.pdf}
  \caption{Illustration of particles in a 2D-space in an early iteration of the PSO algorithm (exploring)}
   \label{fig:psoBeginning}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=4in]{assets/pso_diagram2.pdf}
  \caption{Illustration of particles in a 2D-space in a late iteration of the PSO algorithm (exploiting)}
   \label{fig:psoEnd}
\end{figure}




